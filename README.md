<h1 align='center'>VÂ²Flow: Unifying Visual Tokenization and Large Language Model Vocabularies for Autoregressive Image Generation</h1>




<div align='center'>
    <a href='https://arxiv.org/abs/2503.07493'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>
</div>

<!-- ## ðŸš€ Overview
<p align="center">
  <img src="assets/overview.png" width="720p">
</p> -->

# ðŸ“– Introduction

VÂ²Flow introduces an advanced vector-quantized image tokenizer designed to seamlessly integrate visual tokenization with existing large language model (LLM) vocabularies. By aligning structural representations and latent distributions between image tokens and textual tokens, VÂ²Flow enables effective autoregressive image generation leveraging pre-trained LLMs.

# âœ¨ Highlights

###  1. Structural and Latent Distribution Alignment  with LLM's Vocabulary:



<p align="center">
<img src="./assets/visual_vocabulary_resampler.png" width=100%>
<p>

###  2. Masked Autoregressive Reconstruction from a Flow-matching Perspective:
<p align="center">
<img src="assets/masked_autoregressive_decoder.png" width=100%>
<p>


### 3. Autoregressive Visual Generation on Top of Existing LLMs:


<p align="center">
<img src="assets/generation_results.png" width=100%>
<p>






# ðŸ§© Project Updates
* **2025-03-31:** Release of the complete training and inference codebase for [VÂ²Flow](https://arxiv.org/abs/2503.07493). Pretrained models (1024x1024 and 512x512 resolutions) will be available shortly.
* **2025-03-10:** [VÂ²Flow](https://arxiv.org/abs/2503.07493) is released on arXiv.

# ðŸš€ Training & Inference

## VÂ²Flow Tokenizer
The complete data preparation, training, and inference instructions for the VÂ²Flow tokenizer can be found [here](docs/V2Flow.md).




# ðŸš€ Open-source Plan

- VÂ²Flow tokenizer
  - [x] Training and inference codes 
  - [ ] Checkpoints
  - [ ] Gradio Demo
- VÂ²Flow+LLaMA for Autoregressive Visual Generation
  - [ ] Training and inference codes  
  - [ ] Checkpoints
  - [ ] Gradio Demo



# Acknowledgement

We thank the great work from [MAR](https://github.com/LTH14/mar),  [LLaVA](https://github.com/haotian-liu/LLaVA) and [VideoLLaMA](https://github.com/DAMO-NLP-SG/VideoLLaMA2)